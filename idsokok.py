# -*- coding: utf-8 -*-
"""idsokok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iKGjSSsyGC5VOgU_USpSzPdpvaLBzVDW
"""

import pandas as pd

dataset = pd.read_csv('Smart_Culinary_Training_Data.csv')

dataset.head()

dataset['Category_ID'].unique()

dataset.isnull().sum()

dataset.describe()

from sklearn.preprocessing import OneHotEncoder

# Initialize OneHotEncoder
encoder = OneHotEncoder(drop='first', dtype=int, sparse_output=False)

# Encode the 'Cooking_Method' column
encoded_array = encoder.fit_transform(dataset[['Cooking_Method']])
encoded_columns = encoder.get_feature_names_out(['Cooking_Method'])

# Create a DataFrame from the encoded data
encoded_df = pd.DataFrame(encoded_array, columns=encoded_columns, index=dataset.index)

# Concatenate the new numerical columns to the original dataset
dataset = pd.concat([dataset, encoded_df], axis=1)

# Drop the original text 'Cooking_Method' column (since we now have the numeric version)
# We also drop 'Food_Name' as it is not a trainable feature
dataset.drop(columns=['Cooking_Method', 'Food_Name'], inplace=True)

print("Encoding complete. Current columns:")
print(dataset.columns)

# Define Features (X) - Using ALL available columns
x = dataset.drop(columns=['Target_Final_Calories'])

# Define Target (y)
y = dataset['Target_Final_Calories']

from sklearn.model_selection import train_test_split

# Split the data (80% train, 20% test)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

print("Training data shape:", x_train.shape)
print("Testing data shape:", x_test.shape)
print("Training data shape:", y_train.shape)
print("Testing data shape:", y_test.shape)

import joblib

#addition
joblib.dump(x_train.columns.tolist(), "model_columns.joblib")

from sklearn.ensemble import RandomForestRegressor

# --- IMPROVED MODEL PARAMETERS ---
# Removed 'max_features=1' so the model can see all data (Fat, Weight, etc.)
rf_regressor = RandomForestRegressor(
    n_estimators=200,         # Increased trees for better accuracy
    max_depth=None,           # Let the trees grow deep enough to learn relationships
    min_samples_split=2,      # Allow more precise splits
    random_state=42
)

# Fit the model to the training data
rf_regressor.fit(x_train, y_train)

print("‚úÖ Model retraining complete with improved parameters.")

# Check scores immediately
train_score = rf_regressor.score(x_train, y_train)
test_score = rf_regressor.score(x_test, y_test)
print(f"New Training Score (R¬≤): {train_score:.4f}")
print(f"New Testing Score (R¬≤): {test_score:.4f}")

import xgboost as xgb
import pandas as pd
import matplotlib.pyplot as plt

# ==========================================
# FEATURE IMPORTANCE ANALYSIS
# ==========================================

# 1. Initialize the XGBoost model
# We use this just for analysis to see which columns matter most
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=100,
    random_state=42
)

# 2. Train on your FULL training set (x_train from the previous step)
xgb_model.fit(x_train, y_train)

# 3. Extract feature importance
# 'weight' counts how many times a feature is used to make a decision in the trees
feature_importances = xgb_model.get_booster().get_score(importance_type='weight')

# 4. Create a DataFrame to view the results
importance_df = pd.DataFrame(list(feature_importances.items()),
                             columns=['Feature Name', 'Importance Score'])

# Sort by importance (highest to lowest)
importance_df = importance_df.sort_values(by='Importance Score', ascending=False)

# 5. Print the results
print("üîç Feature Importance Ranking:")
print(importance_df.to_string(index=False))

# Optional: Plot the top 10 features for better visualization
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature Name'].head(10), importance_df['Importance Score'].head(10))
plt.xlabel("Importance Score")
plt.title("Top 10 Factors Affecting Calorie Prediction")
plt.gca().invert_yaxis() # Highest importance at the top
plt.show()

# Calculate scores
train_score = rf_regressor.score(x_train, y_train)
test_score = rf_regressor.score(x_test, y_test)

print(f"Training Score (R¬≤): {train_score:.4f}")
print(f"Testing Score (R¬≤): {test_score:.4f}")

import pandas as pd
import numpy as np

# ==========================================
# 1. GET R¬≤ SCORE
# ==========================================
r2_score = rf_regressor.score(x_test, y_test)
print(f"üìà Model R¬≤ Score: {r2_score:.5f}")
print("-" * 30)

# ==========================================
# 2. GENERATE 100 RANDOM TEST VALUES
# ==========================================
np.random.seed(42) # Fixed seed for reproducibility
n = 100

# Generate base features
synthetic_data = {
    'Category_ID': np.random.randint(1, 26, n),
    'Raw_Weight_g': np.full(n, 100),        # Constant (matches training data)
    'Fat_Added_tbsp': np.random.choice([0, 1, 2], n),
    'Raw_Calories': np.random.uniform(0, 900, n),
    'Raw_Fat_g': np.full(n, 0),             # Constant (matches training data)
    'Target_Final_Weight_g': np.random.uniform(76, 120, n)
}

# Generate Cooking Methods (One-Hot Encoded)
# We pick a random method for each row, then convert it to binary columns
possible_methods = ['Boil', 'Fry', 'Microwave', 'Pan-Sear', 'Roast', 'Steam']
chosen_methods = np.random.choice(possible_methods, n)

# Identify the cooking columns expected by the model
# (We filter columns from x_train that start with 'Cooking_Method_')
model_cooking_cols = [c for c in x_train.columns if 'Cooking_Method_' in c]

# Populate the binary columns (0 or 1)
for col in model_cooking_cols:
    method_name = col.replace('Cooking_Method_', '')
    # If the chosen method matches the column name, put 1, else 0
    synthetic_data[col] = (chosen_methods == method_name).astype(int)

# Create the DataFrame
random_test_df = pd.DataFrame(synthetic_data)

# IMPORTANT: Reorder columns to match exactly what the model was trained on
random_test_df = random_test_df[x_train.columns]

# ==========================================
# 3. PREDICT & DISPLAY RESULTS
# ==========================================
predictions = rf_regressor.predict(random_test_df)

# Create a clean results table
results_df = random_test_df.copy()
results_df['PREDICTED_CALORIES'] = predictions

# Select specific columns to display (matching your request)
display_cols = [
    'Category_ID',
    'Raw_Calories',
    'Target_Final_Weight_g',
    'Fat_Added_tbsp',
    'PREDICTED_CALORIES'
]

print("üìä Final Prediction Results (First 10 shown):")
print(results_df[display_cols].head(10).to_string(index=False))

# If you want to see all 100, you can use:
# print(results_df[display_cols].to_string(index=False))

"""For ui

"""

!pip install streamlit pyngrok

from pyngrok import ngrok
ngrok.set_auth_token("36EvfgGvZ9MWB9KOITaveXwO7gX_6Bdod216rmTmFE7r9xuSH")

import joblib

# Save the new, smarter model
joblib.dump(rf_regressor, "rf_model.joblib")
joblib.dump(x_train.columns, "model_columns.joblib") # Ensure columns are saved
joblib.dump(encoder, "encoder.joblib")

print("‚úÖ New model and system files saved!")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import pickle
# import joblib
# from sklearn.preprocessing import OneHotEncoder
# 
# # ======================================
# # CATEGORY LIST (1‚Äì25)
# # ======================================
# CATEGORY_MAP = {
#     1: "100 Dairy and Egg Products",
#     2: "200 Spices and Herbs",
#     3: "300 Baby Foods",
#     4: "400 Fats and Oils",
#     5: "500 Poultry Products",
#     6: "600 Soups, Sauces, and Gravies",
#     7: "700 Sausages and Luncheon Meats",
#     8: "800 Breakfast Cereals",
#     9: "900 Fruits and Fruit Juices",
#     10: "1000 Pork Products",
#     11: "1100 Vegetables and Vegetable Products",
#     12: "1200 Nut and Seed Products",
#     13: "1300 Beef Products",
#     14: "1400 Beverages",
#     15: "1500 Finfish and Shellfish Products",
#     16: "1600 Legumes and Legume Products",
#     17: "1700 Lamb, Veal, and Game Products",
#     18: "1800 Baked Products",
#     19: "1900 Sweets",
#     20: "2000 Cereal Grains and Pasta",
#     21: "2100 Fast Foods",
#     22: "2200 Meals, Entrees, and Side Dishes",
#     23: "2500 Snacks",
#     24: "3500 American Indian/Alaska Native Foods",
#     25: "3600 Restaurant Foods"
# }
# 
# # ======================================
# # STREAMLIT UI
# # ======================================
# st.title("üçΩ Smart Culinary Calorie Predictor")
# 
# st.subheader("üìå Food Category Guide (1‚Äì25)")
# for key, value in CATEGORY_MAP.items():
#     st.write(f"**{key}. {value}**")
# 
# st.markdown("---")
# 
# # ======================================
# # USER INPUT
# # ======================================
# st.header("üî¢ Enter Food Details")
# 
# category_id = st.number_input("Select Food Category (1‚Äì25)", min_value=1, max_value=25, step=1)
# 
# raw_calories = st.number_input("Raw Calories (kcal)", min_value=0.0, format="%.2f")
# raw_weight = st.number_input("Raw Weight (g)", min_value=0.0, format="%.2f")
# raw_fat = st.number_input("Raw Fat (g)", min_value=0.0, format="%.2f")
# fat_added_tbsp = st.number_input("Fat Added (tablespoons)", min_value=0.0, format="%.2f")
# target_final_weight = st.number_input("Target Final Cooked Weight (g)", min_value=0.0, format="%.2f")
# 
# cooking_method = st.selectbox(
#     "Select Cooking Method",
#     ["Boil", "Steam", "Deep_Fry", "Pan_Fry", "Bake", "Roast", "Air_Fry", "Microwave"]
# )
# 
# # ======================================
# # LOAD MODEL + ENCODER + MODEL COLUMNS
# # ======================================
# @st.cache_resource
# def load_artifacts():
#     # Load encoder training dataset
#     dataset = pd.read_csv("Smart_Culinary_Training_Data.csv")
# 
#     encoder = OneHotEncoder(drop="first", sparse_output=False)
#     encoder.fit(dataset[["Cooking_Method"]])
# 
#     # Load trained RF model
#     with open("rf_model.pkl", "rb") as f:
#         model = pickle.load(f)
# 
#     # Load correct column order
#     model_columns = joblib.load("model_columns.joblib")
# 
#     return model, encoder, model_columns
# 
# model, encoder, model_columns = load_artifacts()
# 
# # ======================================
# # PREDICTION
# # ======================================
# if st.button("Predict Final Calories"):
#     try:
#         # Build raw input dict
#         input_data = {
#             "Category_ID": category_id,
#             "Raw_Weight_g": raw_weight,
#             "Fat_Added_tbsp": fat_added_tbsp,
#             "Raw_Calories": raw_calories,
#             "Raw_Fat_g": raw_fat,
#             "Target_Final_Weight_g": target_final_weight,
#         }
# 
#         df_input = pd.DataFrame([input_data])
# 
#         # Encode cooking method
#         encoded_df = pd.DataFrame(
#             encoder.transform([[cooking_method]]),
#             columns=encoder.get_feature_names_out(["Cooking_Method"])
#         )
# 
#         # Merge numerical + encoded
#         final_df = pd.concat([df_input, encoded_df], axis=1)
# 
#         # Reorder columns to EXACT training order
#         final_df = final_df.reindex(columns=model_columns, fill_value=0)
# 
#         prediction = model.predict(final_df)[0]
# 
#         st.success(f"üî• Estimated Final Calories: **{prediction:.2f} kcal**")
# 
#     except Exception as e:
#         st.error(f"‚ùå Error: {str(e)}")
#

!streamlit run app.py &>/dev/null&

import joblib
joblib.dump(rf_regressor, "rf_model.joblib")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# # =====================================================
# # CONFIGURATION
# # =====================================================
# st.set_page_config(
#     page_title="Smart Culinary Predictor",
#     page_icon="üçΩ",
#     layout="centered"
# )
# 
# # =====================================================
# # RESOURCE LOADING
# # =====================================================
# @st.cache_resource
# def load_resources():
#     dataset = pd.read_csv("Smart_Culinary_Training_Data.csv")
#     model = joblib.load("rf_model.joblib")
#     encoder = joblib.load("encoder.joblib")
#     model_columns = joblib.load("model_columns.joblib")
#     return dataset, model, encoder, model_columns
# 
# try:
#     dataset, model, encoder, model_columns = load_resources()
# except Exception as e:
#     st.error(f"Error loading resources: {e}")
#     st.stop()
# 
# # =====================================================
# # UI: HEADER & MODE SELECTION
# # =====================================================
# st.title("üçΩ Smart Culinary Calorie Predictor")
# st.markdown("Calculate final calories based on **Food**, **Portion Size**, **Added Fat**, and **Cooking Method**.")
# st.markdown("---")
# 
# # Ask the user: Do you want to use our database or your own numbers?
# input_mode = st.radio(
#     "How would you like to enter data?",
#     ["Select from Food List", "Enter Custom Manual Data"],
#     horizontal=True
# )
# 
# st.markdown("---")
# 
# # =====================================================
# # UI: DYNAMIC INPUTS
# # =====================================================
# col1, col2 = st.columns(2)
# 
# # --- LEFT COLUMN: FOOD DETAILS ---
# with col1:
#     st.subheader("1. Food Details")
# 
#     if input_mode == "Select from Food List":
#         # MODE A: Database Lookup
#         food_name = st.selectbox("Select Food Item", sorted(dataset["Food_Name"].unique()))
# 
#         # Look up reference values
#         ref_row = dataset[dataset["Food_Name"] == food_name].iloc[0]
#         base_ref_weight = float(ref_row["Raw_Weight_g"])
#         base_ref_cal = float(ref_row["Raw_Calories"])
#         base_ref_fat = float(ref_row["Raw_Fat_g"])
#         base_cat_id = ref_row["Category_ID"]
# 
#         # User adjusts the weight only
#         user_weight = st.number_input("Raw Weight (g)", min_value=1.0, value=base_ref_weight, step=10.0, format="%.2f")
# 
#         # Calculate Input Calories/Fat based on weight ratio
#         scale_factor = user_weight / base_ref_weight if base_ref_weight > 0 else 1.0
# 
#         # These are the values we will send to the logic
#         final_input_calories = base_ref_cal * scale_factor
#         final_input_fat = base_ref_fat * scale_factor
# 
#         # Display the auto-calculated values for transparency
#         st.info(f"üìä Auto-Calculated Base Content:\n\n**{final_input_calories:.0f} kcal** | **{final_input_fat:.1f}g fat**")
# 
#     else:
#         # MODE B: Manual Entry
#         # User types everything manually
#         user_weight = st.number_input("Raw Weight (g)", min_value=1.0, value=100.0, step=10.0)
#         final_input_calories = st.number_input("Total Raw Calories (kcal)", min_value=0.0, value=100.0, step=10.0)
#         final_input_fat = st.number_input("Total Raw Fat (g)", min_value=0.0, value=0.0, step=1.0)
# 
#         # For manual entry, we default to Category 11 (Vegetables) or 22 (Meals) as a generic fallback
#         # because the user didn't pick a specific food category.
#         base_cat_id = 11
# 
# # --- RIGHT COLUMN: COOKING DETAILS ---
# with col2:
#     st.subheader("2. Cooking Details")
# 
#     # Cooking Method
#     cooking_method = st.selectbox("Cooking Method", encoder.categories_[0])
# 
#     # Fat Added
#     fat_added_tbsp = st.number_input("Fat Added (tbsp)", min_value=0.0, value=0.0, step=0.5, format="%.2f")
# 
# # =====================================================
# # HYBRID PREDICTION LOGIC
# # =====================================================
# if st.button("üî• Predict Final Calories", type="primary"):
#     try:
#         # Estimate Final Weight (Heuristic: most foods retain 80-90% weight or gain water)
#         # We use a rough estimate since we don't have a DB reference for custom foods
#         estimated_final_weight = user_weight * 0.9
# 
#         # 1. ENCODE COOKING METHOD
#         encoded_matrix = encoder.transform([[cooking_method]])
#         encoded_df = pd.DataFrame(encoded_matrix, columns=encoder.get_feature_names_out(['Cooking_Method']))
# 
#         # 2. AI PREDICTION (BASE FOOD ONLY)
#         # We use the Hybrid trick: Send Fat_Added = 0 to the AI model
#         input_dict = {
#             "Category_ID": base_cat_id,
#             "Raw_Calories": final_input_calories,   # User provided or Database scaled
#             "Raw_Fat_g": final_input_fat,           # User provided or Database scaled
#             "Raw_Weight_g": user_weight,
#             "Fat_Added_tbsp": 0.0,                  # Force 0 for base prediction
#             "Target_Final_Weight_g": estimated_final_weight
#         }
# 
#         full_input = pd.concat([pd.DataFrame([input_dict]), encoded_df], axis=1)
# 
#         # Reindex to match training columns perfectly
#         full_input = full_input.reindex(columns=model_columns, fill_value=0)
# 
#         # Get AI prediction for the food itself
#         base_prediction = model.predict(full_input)[0]
# 
#         # 3. MANUAL MATH FOR FAT (1 tbsp = ~120 kcal)
#         fat_calories_math = fat_added_tbsp * 120.0
# 
#         # 4. FINAL SUM
#         final_prediction = base_prediction + fat_calories_math
# 
#         # =====================================================
#         # DISPLAY RESULTS
#         # =====================================================
#         st.success(f"### Estimated Final Calories: {final_prediction:,.0f} kcal")
# 
#         with st.expander("‚ÑπÔ∏è  See Calculation Breakdown"):
#             st.write("Calculation Logic:")
#             c1, c2, c3 = st.columns(3)
#             c1.metric("Base Raw Calories", f"{final_input_calories:.0f}")
#             c2.metric("Fat Added (Manual)", f"+{fat_calories_math:.0f} kcal")
#             c3.metric("Predicted Total", f"{final_prediction:.0f} kcal")
# 
#     except Exception as e:
#         st.error(f"Prediction Error: {e}")

!pkill ngrok

public_url = ngrok.connect(8501)
public_url